{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Embeddings and Next Sentence Prediction"
      ],
      "metadata": {
        "id": "z3ppiJEU3OFS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhtSpBdJH4ZG"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import re\n",
        "import random\n",
        "import transformers, datasets\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import itertools\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "xa8YInCePoEm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean data**"
      ],
      "metadata": {
        "id": "lhEhWoQXdATa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(text: str):    \n",
        "    pattern = re.compile( \n",
        "        r'\\*\\*\\d+\\*\\*|'  \n",
        "        r'\\(#\\w+\\)|'  \n",
        "        r'[\\*]|'  \n",
        "        r'\\b\\w+\\.(?:json)\\b|'  \n",
        "        r'-{2,}|'  \n",
        "        r'>(?:\\s+)?|'  \n",
        "        r'<[^>]*>|'  \n",
        "        r\"!\\S+\\.(?:jpg|jpeg|gif|png)|\"  \n",
        "        r'IMAGENIMAGEN|'  \n",
        "        r'(!image\\.png)|'  \n",
        "        r'(!untitled\\.png)|'\n",
        "        r'(/\\.attachments/untitled-[a-zA-Z0-9\\-]+\\.png)|'\n",
        "        r'(/\\.attachments/image-[a-zA-Z0-9\\-]+\\.png)|'\n",
        "        r'!media/[a-zA-Z0-9]+\\.png|'\n",
        "        r'image\\d+\\.png|'\n",
        "        r'[^\\w\\s\\\\p{L}]+'\n",
        "        r'([a-zA-ZáéíóúÁÉÍÓÚñÑüÜ]+)(https?://)|',     \n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    \n",
        "    clean_text = pattern.sub('', text.lower())    \n",
        "    clean_text = re.sub(r\"!\\S+\\.(?:jpg|jpeg|gif|png)\", \"IMAGEN\", clean_text)\n",
        "    clean_text = re.sub(r'\\s\\)\\s', ' ', clean_text)\n",
        "    clean_text = re.sub(r'<[^>]+>', '', clean_text)\n",
        "\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "mWgbovUJdCGE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunk data**"
      ],
      "metadata": {
        "id": "XqtZ4EE5uai2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('./datasets')"
      ],
      "metadata": {
        "id": "x9BIFbUdjPBL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "corpus_wiki_data = \"./datasets/corpus_wtout_nlines.txt\"\n",
        "wiki_lines = list()\n",
        "\n",
        "with open(corpus_wiki_data, 'r') as w:\n",
        "  lines = w.readlines()\n",
        "    \n",
        "def chunk_text(texto, MAX_LEN=MAX_LEN):  \n",
        "    if len(texto) <= MAX_LEN:\n",
        "        return texto\n",
        "    else:\n",
        "        split_texts = []\n",
        "        num_splits = math.ceil(len(texto) // MAX_LEN)\n",
        "        for i in range(num_splits):\n",
        "            split_text = texto[i * MAX_LEN: (i+1) * MAX_LEN]\n",
        "            split_texts.append(split_text)\n",
        "        if len(texto) % MAX_LEN != 0:\n",
        "            split_text = texto[num_splits * MAX_LEN:]\n",
        "            split_texts.append(split_text)\n",
        "        return split_texts\n",
        "\n",
        "for line in lines:\n",
        "  clean_text = process_text(line)\n",
        "  batch_line = chunk_text(clean_text, MAX_LEN)\n",
        "  wiki_lines.append(batch_line)"
      ],
      "metadata": {
        "id": "aYmuTRppoSXg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_lines[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmqbjGf0U9yn",
        "outputId": "93eb5a96-a02e-4571-97e3-f6d107055b20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[[_toc_]]\\n',\n",
              " '# 1. introducción\\n',\n",
              " 'en esta sección se encontrará información sobre que debes tener en cuenta para poder realizar las pruebas de manera local y que están integradas con adp y en ambientesbc.\\n',\n",
              " '# 2. consideraciones\\n',\n",
              " ['- aplica para pruebas como: [performance modular](https://grupobancolombia.visualstudio.com/vicepresidencia%20servicios%20de%20tecnolog%c3%ada/_wiki/wikis/vicepresidencia%20servicios%20de%20tecnolog%c3%ada.wiki/1988/proceso-para-realizar-pruebas-de-performance-de-componente-modulares) , [aceptación](https://grupobancolombia.visualstudio.com/vicepresidencia%20servicios%20de%20tecnolog%c3%ada/_wiki/wikis/vicepresidencia%20servicios%20de%20tecnolog%c3%ada.wiki/9573/pruebas-de-integraci%c3%b3n-(acceptancetest))',\n",
              "  ' y funcionales e2e.\\n']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting \n",
        "lines_dic = {}\n",
        "for line in range(len(wiki_lines)):  \n",
        "  lines_dic[f\"L{line}\"] = wiki_lines[line]\n",
        "\n",
        "# Pares QA\n",
        "pairs = []\n",
        "for line in range(len(wiki_lines)):  \n",
        "  qa_pairs = list()\n",
        "  if line == len(wiki_lines) - 1:\n",
        "    break\n",
        "\n",
        "  first = lines_dic[f\"L{line}\"]\n",
        "  second = lines_dic[f\"L{line+1}\"]\n",
        "\n",
        "  qa_pairs.append(first)\n",
        "  qa_pairs.append(second)\n",
        "  pairs.append(qa_pairs)\n",
        "\n",
        "# Data type str\n",
        "for i in range(len(pairs)):  \n",
        "  for j in range(len(pairs[i])):\n",
        "    if type(pairs[i][j]) == list:      \n",
        "      pairs[i][j] = \" \".join(pairs[i][j])    "
      ],
      "metadata": {
        "id": "6RZYa_KmzLqi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[105]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcAAzTn51vJm",
        "outputId": "2d3f2ccc-b74f-473f-eff1-fe26d761d7fe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4. [¿cuál es la politica de calidad de devops que se debe tener en cuenta en las etapas iniciales?](https://grupobancolombia.visualstudio.com/vicepresidencia%20servicios%20de%20tecnolog%c3%ada/_wiki/wikis/vicepresidencia%20servicios%20de%20tecnolog%c3%ada.wiki/12944/pol%c3%adtica-de-calidad-de-devops)\\n',\n",
              " '5. [¿cómo reservo el ambiente de pruebas?](https://grupobancolombia.visualstudio.com/vicepresidencia%20servicios%20de%20tecnolog%c3%ada/_wiki/wikis/vicepresidencia%20servicios%20de%20tecnolog%c3%ada.wiki/4877/como-crear-una-reserva)\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "tCYxuvr-3SRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WordPiece Tokenization**"
      ],
      "metadata": {
        "id": "5I6DqKwo3XEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('./data')"
      ],
      "metadata": {
        "id": "b7A9PfEl9clX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = []\n",
        "file_count = 0\n",
        "\n",
        "for sample in tqdm.tqdm([x[0] for x in pairs]):  \n",
        "  text_data.append(sample)\n",
        "\n",
        "  if len(text_data) == 1000:\n",
        "    with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
        "      fp.write('\\n'.join(text_data))\n",
        "    text_data = []\n",
        "    file_count += 1"
      ],
      "metadata": {
        "id": "Tx3y0PC4A-jI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24b34df-ad4f-48c6-f5d8-6af46cdd4cc6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4928/4928 [00:00<00:00, 403260.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paths = [str(x) for x in Path('./data').glob('**/*.txt')]\n",
        "\n",
        "# Entrenando nuestro propio tokenizer\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False,\n",
        "    strip_accents=False,\n",
        "    lowercase=True\n",
        ")\n",
        "tokenizer.train(\n",
        "    files=paths,\n",
        "    vocab_size=30_000,\n",
        "    min_frequency=2,\n",
        "    limit_alphabet=1000,\n",
        "    wordpieces_prefix='',\n",
        "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
        ")\n",
        "\n",
        "os.mkdir('./bert-it-1')\n",
        "tokenizer.save_model('./bert-it-1', 'bert-it')\n",
        "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74cQrWUH2ux6",
        "outputId": "e7232930-4f5b-4f04-88fe-8bf24c95c473"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1713: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "  def __init__(self, data_pair, tokenizer, seq_len=64):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.seq_len = seq_len\n",
        "    self.corpus_lines = len(data_pair)\n",
        "    self.lines = data_pair\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.corpus_lines\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    \n",
        "    t1, t2, is_next_label = self.get_sent(item)\n",
        "\n",
        "    t1_random, t1_label = self.random_word(t1)\n",
        "    t2_random, t2_label = self.random_word(t2)\n",
        "\n",
        "    t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
        "    t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
        "    t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
        "    t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
        "\n",
        "    segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
        "    bert_input = (t1 + t2)[:self.seq_len]\n",
        "    bert_label = (t1_label + t2_label)[:self.seq_len]\n",
        "    padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
        "    bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
        "\n",
        "    output = {\"bert_input\": bert_input,\n",
        "              \"bert_label\": bert_label,\n",
        "              \"segment_label\": segment_label,\n",
        "              \"is_next\": is_next_label}\n",
        "\n",
        "    return {key: torch.tensor(value) for key, value in output.items()}\n",
        "\n",
        "  def random_word(self, sentence):\n",
        "    tokens = sentence.split()\n",
        "    output_label = []\n",
        "    output = []\n",
        "    # 15% of the tokens\n",
        "    for i, token in enumerate(tokens):\n",
        "        prob = random.random()\n",
        "        # remove cls and sep token\n",
        "        token_id = self.tokenizer(token)['input_ids'][1:-1]\n",
        "        if prob < 0.15:\n",
        "            prob /= 0.15\n",
        "            # 80% \n",
        "            if prob < 0.8:\n",
        "                for i in range(len(token_id)):\n",
        "                    output.append(self.tokenizer.vocab['[MASK]'])\n",
        "            # 10% \n",
        "            elif prob < 0.9:\n",
        "                for i in range(len(token_id)):\n",
        "                    output.append(random.randrange(len(self.tokenizer.vocab)))\n",
        "            # 10% \n",
        "            else:\n",
        "                output.append(token_id)\n",
        "            output_label.append(token_id)\n",
        "        else:\n",
        "            output.append(token_id)\n",
        "            for i in range(len(token_id)):\n",
        "                output_label.append(0)\n",
        "\n",
        "    # flattening\n",
        "    output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
        "    output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
        "    assert len(output) == len(output_label)\n",
        "    return output, output_label\n",
        "\n",
        "  def get_sent(self, index):\n",
        "    t1, t2 = self.get_corpus_line(index)\n",
        "\n",
        "    if random.random() > 0.5:\n",
        "      return t1, t2, 1\n",
        "    else:\n",
        "      return t1, self.get_random_line(), 0\n",
        "  \n",
        "  def get_corpus_line(self, item):\n",
        "    return self.lines[item][0], self.lines[item][1]\n",
        "  \n",
        "  def get_random_line(self):\n",
        "    return self.lines[random.randrange(len(self.lines))][1]"
      ],
      "metadata": {
        "id": "S0l5_xLhBSH2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = BERTDataset(\n",
        "    pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
        "sample_data = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "p6wKjQBf3CaL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[random.randrange(len(train_data))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WMmHENi3TLN",
        "outputId": "30a9e355-0c11-4569-9089-887852ebad16"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert_input': tensor([   1,   16,   35,   37,    3,    3,   38,  336,    4,    3,  115,   36,\n",
              "          130,  406,   38,    2,   16,  141,  111, 2158,  205, 5032, 1941,   40,\n",
              "          149, 3950,   95,    3, 1803,  829,   17,  319,  127,  149, 1609,    4,\n",
              "         2882,   40, 1609, 4909,   17,    2,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " 'bert_label': tensor([   0,    0,    0,    0, 2247,   29,    0,    0,    0,   94,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,  112,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " 'segment_label': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'is_next': tensor(1)}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "7_2ifaFM4a_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Positional encodings\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        for pos in range(max_len):   \n",
        "            \n",
        "            for i in range(0, d_model, 2):   \n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        \n",
        "        self.pe = pe.unsqueeze(0)        \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe\n",
        "\n",
        "class BERTEmbedding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BERT Embedding which is consisted with under features\n",
        "        1. TokenEmbedding : normal embedding matrix\n",
        "        2. PositionalEmbedding : adding positional information using sin, cos\n",
        "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
        "        sum of all these features are output of BERTEmbedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, seq_len=512, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param vocab_size: total vocab size\n",
        "        :param embed_size: embedding size of token embedding\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        # (m, seq_len) --> (m, seq_len, embed_size)        \n",
        "        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0)\n",
        "        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "       \n",
        "    def forward(self, sequence, segment_label):\n",
        "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "xmyuPIFl3pSl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### attention layers\n",
        "class MultiHeadedAttention(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        \n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.query = torch.nn.Linear(d_model, d_model)\n",
        "        self.key = torch.nn.Linear(d_model, d_model)\n",
        "        self.value = torch.nn.Linear(d_model, d_model)\n",
        "        self.output_linear = torch.nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, d_model)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, d_model)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
        "\n",
        "        # (batch_size, h, max_len, max_len)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    \n",
        "\n",
        "        # (batch_size, h, max_len, max_len)\n",
        "        # Attention weight for all non-pad tokens        \n",
        "        weights = F.softmax(scores, dim=-1)           \n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n",
        "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "\n",
        "        # (batch_size, max_len, d_model)\n",
        "        return self.output_linear(context)\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "\n",
        "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.activation = torch.nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        d_model=768,\n",
        "        heads=12, \n",
        "        feed_forward_hidden=768 * 4, \n",
        "        dropout=0.1\n",
        "        ):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = torch.nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        # embeddings: (batch_size, max_len, d_model)\n",
        "        # encoder mask: (batch_size, 1, 1, max_len)\n",
        "        # result: (batch_size, max_len, d_model)\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        # residual layer\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        \n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "Ihe57tBf6Cqv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param vocab_size: vocab_size of total words\n",
        "        :param hidden: BERT model hidden size\n",
        "        :param n_layers: numbers of Transformer blocks(layers)\n",
        "        :param attn_heads: number of attention heads\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.heads = heads\n",
        "\n",
        "        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n",
        "        self.feed_forward_hidden = d_model * 4\n",
        "\n",
        "        # embedding for BERT, sum of positional, segment, token embeddings\n",
        "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)\n",
        "\n",
        "        # multi-layers transformer blocks, deep network\n",
        "        self.encoder_blocks = torch.nn.ModuleList(\n",
        "            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, segment_info):\n",
        "        # attention masking for padded token\n",
        "        # (batch_size, 1, seq_len, seq_len)\n",
        "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
        "\n",
        "        # embedding the indexed sequence to sequence of vectors\n",
        "        x = self.embedding(x, segment_info)\n",
        "\n",
        "        # running over multiple transformer blocks\n",
        "        for encoder in self.encoder_blocks:\n",
        "            x = encoder.forward(x, mask)\n",
        "        return x\n",
        "\n",
        "class NextSentencePrediction(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    2-class classification model : is_next, is_not_next\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden):\n",
        "        \"\"\"\n",
        "        :param hidden: BERT model output size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(hidden, 2)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # use only the first token which is the [CLS]\n",
        "        return self.softmax(self.linear(x[:, 0]))\n",
        "\n",
        "class MaskedLanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    predicting origin token from masked input sequence\n",
        "    n-class classification problem, n-class = vocab_size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden, vocab_size):\n",
        "        \"\"\"\n",
        "        :param hidden: output size of BERT model\n",
        "        :param vocab_size: total vocab size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(hidden, vocab_size)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.softmax(self.linear(x))\n",
        "\n",
        "class BERTLM(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BERT Language Model\n",
        "    Next Sentence Prediction Model + Masked Language Model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bert: BERT, vocab_size):\n",
        "        \"\"\"\n",
        "        :param bert: BERT model which should be trained\n",
        "        :param vocab_size: total vocab size for masked_lm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.next_sentence = NextSentencePrediction(self.bert.d_model)\n",
        "        self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, segment_label):\n",
        "        x = self.bert(x, segment_label)\n",
        "        return self.next_sentence(x), self.mask_lm(x)"
      ],
      "metadata": {
        "id": "OUWaf3ZyK9KL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScheduledOptim():\n",
        "    \"\"\"Wrapper class for learning rate scheduling\"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = 0\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):        \n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):        \n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        return np.min([\n",
        "            np.power(self.n_current_steps, -0.5),\n",
        "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "HkcGyxO8NKay"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTTrainer:\n",
        "    def __init__(\n",
        "        self, \n",
        "        model, \n",
        "        train_dataloader, \n",
        "        test_dataloader=None, \n",
        "        lr= 1e-4,\n",
        "        weight_decay=0.01,\n",
        "        betas=(0.9, 0.999),\n",
        "        warmup_steps=10000,\n",
        "        log_freq=10,\n",
        "        device='cuda'\n",
        "        ):\n",
        "\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.train_data = train_dataloader\n",
        "        self.test_data = test_dataloader\n",
        "\n",
        "        # Setting the Adam optimizer with hyper-param\n",
        "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        self.optim_schedule = ScheduledOptim(\n",
        "            self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps\n",
        "            )\n",
        "\n",
        "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
        "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
        "        self.log_freq = log_freq\n",
        "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
        "    \n",
        "    def train(self, epoch):\n",
        "        self.iteration(epoch, self.train_data)\n",
        "\n",
        "    def test(self, epoch):\n",
        "        self.iteration(epoch, self.test_data, train=False)\n",
        "\n",
        "    def iteration(self, epoch, data_loader, train=True):\n",
        "        \n",
        "        avg_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_element = 0\n",
        "        \n",
        "        mode = \"train\" if train else \"test\"\n",
        "\n",
        "        # progress bar\n",
        "        data_iter = tqdm.tqdm(\n",
        "            enumerate(data_loader),\n",
        "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
        "            total=len(data_loader),\n",
        "            bar_format=\"{l_bar}{r_bar}\"\n",
        "        )\n",
        "\n",
        "        for i, data in data_iter:\n",
        "\n",
        "            # batch_data will be sent into the device(GPU or cpu)\n",
        "            data = {key: value.to(self.device) for key, value in data.items()}\n",
        "\n",
        "            # forward the next_sentence_prediction and masked_lm model\n",
        "            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
        "\n",
        "            # NLL(negative log likelihood) loss of is_next classification result\n",
        "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
        "            \n",
        "            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n",
        "            # criterion(mask_lm_output.view(-1, mask_lm_output.size(-1)), data[\"bert_label\"].view(-1))\n",
        "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
        "\n",
        "            # Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
        "            loss = next_loss + mask_loss\n",
        "\n",
        "            # backward and optimization only in train\n",
        "            if train:\n",
        "                self.optim_schedule.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optim_schedule.step_and_update_lr()\n",
        "\n",
        "            # next sentence prediction accuracy\n",
        "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
        "            avg_loss += loss.item()\n",
        "            total_correct += correct\n",
        "            total_element += data[\"is_next\"].nelement()\n",
        "\n",
        "            post_fix = {\n",
        "                \"epoch\": epoch,\n",
        "                \"iter\": i,\n",
        "                \"avg_loss\": avg_loss / (i + 1),\n",
        "                \"avg_acc\": total_correct / total_element * 100,\n",
        "                \"loss\": loss.item()\n",
        "            }\n",
        "\n",
        "            if i % self.log_freq == 0:\n",
        "                data_iter.write(str(post_fix))\n",
        "        print(\n",
        "            f\"EP{epoch}, {mode}: \\\n",
        "            avg_loss={avg_loss / len(data_iter)}, \\\n",
        "            total_acc={total_correct * 100.0 / total_element}\"\n",
        "        ) "
      ],
      "metadata": {
        "id": "zozvY9YgN4Jc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = BERTDataset(\n",
        "   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "   train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
        "\n",
        "bert_model = BERT(\n",
        "  vocab_size=len(tokenizer.vocab),\n",
        "  d_model=768,\n",
        "  n_layers=2,\n",
        "  heads=12,\n",
        "  dropout=0.1\n",
        ")\n",
        "\n",
        "bert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n",
        "bert_trainer = BERTTrainer(bert_lm, train_loader, device=\"cpu\")\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  bert_trainer.train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MROyF1RbO7tr",
        "outputId": "36a06a2f-f9aa-47f6-b03d-8ccab29e91d9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 27307105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   1%|| 1/154 [01:02<2:39:17, 62.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 0, 'avg_loss': 9.557798385620117, 'avg_acc': 37.5, 'loss': 9.557798385620117}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   7%|| 11/154 [11:12<2:28:10, 62.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 10, 'avg_loss': 9.522670659151943, 'avg_acc': 51.13636363636363, 'loss': 9.480085372924805}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  14%|| 21/154 [21:11<2:15:11, 60.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 20, 'avg_loss': 9.489825657435826, 'avg_acc': 50.44642857142857, 'loss': 9.434412002563477}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  20%|| 31/154 [31:17<2:04:53, 60.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 30, 'avg_loss': 9.461046126581007, 'avg_acc': 49.69758064516129, 'loss': 9.392536163330078}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  27%|| 41/154 [41:54<2:03:48, 65.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 40, 'avg_loss': 9.423962895463152, 'avg_acc': 50.381097560975604, 'loss': 9.2989501953125}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  33%|| 51/154 [52:14<1:46:46, 62.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 50, 'avg_loss': 9.386271663740569, 'avg_acc': 50.55147058823529, 'loss': 9.237812995910645}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  40%|| 61/154 [1:02:26<1:36:41, 62.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 60, 'avg_loss': 9.35043177057485, 'avg_acc': 50.0, 'loss': 9.187257766723633}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  46%|| 71/154 [1:12:38<1:25:32, 61.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 70, 'avg_loss': 9.317107012574102, 'avg_acc': 50.220070422535215, 'loss': 9.130542755126953}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  53%|| 81/154 [1:22:44<1:14:20, 61.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 80, 'avg_loss': 9.282886622864524, 'avg_acc': 50.27006172839506, 'loss': 9.056076049804688}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  59%|| 91/154 [1:32:59<1:05:34, 62.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 90, 'avg_loss': 9.24680620759398, 'avg_acc': 49.96565934065934, 'loss': 8.895912170410156}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  66%|| 101/154 [1:43:24<55:18, 62.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 100, 'avg_loss': 9.210352567162845, 'avg_acc': 50.21658415841584, 'loss': 8.820825576782227}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  72%|| 111/154 [1:53:29<43:24, 60.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 110, 'avg_loss': 9.172703665656012, 'avg_acc': 50.05630630630631, 'loss': 8.730839729309082}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  79%|| 121/154 [2:03:36<33:43, 61.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 120, 'avg_loss': 9.134493583490041, 'avg_acc': 50.05165289256198, 'loss': 8.737908363342285}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  85%|| 131/154 [2:13:23<22:27, 58.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 130, 'avg_loss': 9.09056054180815, 'avg_acc': 50.0, 'loss': 8.479402542114258}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  92%|| 141/154 [2:23:20<12:58, 59.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 140, 'avg_loss': 9.047352621741329, 'avg_acc': 50.310283687943254, 'loss': 8.420921325683594}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:  98%|| 151/154 [2:33:31<03:01, 60.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 150, 'avg_loss': 9.00233828310935, 'avg_acc': 50.18625827814569, 'loss': 8.34437084197998}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0: 100%|| 154/154 [2:36:29<00:00, 60.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP0, train:             avg_loss=8.988147760366465,             total_acc=50.10146103896104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}